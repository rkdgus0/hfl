[FL_Service]
service_id = 0  ; 서비스 식별자
leader = 1      ; 리더 수
aggregator = 10 ; Aggregator 수
userset = 100   ; 유저셋 수
user = 100      ; 유저 수
; simulate(True, False) : True시, 계층간 통신 없는 시뮬레이션 실행
simulate = True
; end_condition[ROUND, ACC(%)] : 몇 ROUND, ACC 몇 % 에서 멈출지 선택 
end_condition = 500, 100

# ====== Model & Data Setting ======
; model(mcmahan2NN, mcmahanCNN, resnet50, resnet101, densenet121, VGG16) : 모델 세팅
model = densenet121
; pretrained(True, False) : 모델의 pretrained 여부(resnet50/101, densenet121, VGG16에서만 작동)
pretrained = True
optimizer = adam    ;Optimizer 세팅(adam, SGD)
dataset = cifar10   ; 데이터셋 세팅(mnist, cifar10)
data_split = diff   ;데이터 분산 방법(random, iid, non_iid, diri, diff)
; diri_alpha(0 초과, 30 미만 값 권장) : (data_split : diri)에서의 편향도 조절값(작을수록 편향도가 커진다)
diri_alpha = 1
# ====== Aggregation Method Setting ======
hybrid = True
agg_method = Fedavg, Acc, F1macro
adaptive_agg_method = no_adapt
adaptive_parameter = 40
# ====== Hyper parameter Setting ======
; aggregator_round : 유저셋의 학습 모델을 모으는 횟수
aggregator_round = 1
local_epoch = 5             ;유저셋의 학습 횟수
local_batch_size = 64       ;모델 학습시 batch size
local_learning_rate = 0.001 ;모델 학습시 learning rate
evaluate_batch_size = 64    ;모델 평가시 batch size
lr_decay = 0.99
lr_decay_round = 5
# ====== Simulator parameter Setting ======
classification = n_data
; agg_delays(aggregator 개수만큼 입력) : aggregator의 티어(글로벌 업데이트 지연 라운드)
agg_delays = 0,0,0,0,0,0,0,0,0,0
; client_mapping(Equal, diff) : aggregator마다 유저수를 Equal/different하게 분배
user_mapping = equal
; delay_method(Range, Fixed) : (Fixed) agg_delays가 매 라운드 동등, (Range) agg_delays가 매 라운드마다 delay_range 사이 에서 정해진다
delay_method = Fixed
delay_range = 2
delay_epoch = 0 ;해당 글로벌 업데이트에 참여하지 못한 aggregator가 추가로 수행할 로컬 업데이트 횟수
eval_every = 1  ;글로벌 Test Acc를 산출하는 주기(round)
model_decay = Equal; model_decay(Equal, Frac) : ??

# ====== Wandb Setting ======
wandb = True
wandb_id = create0327
wandb_api = b2f21ce10a4365a21cfce06ad41f9a7f23d34639
group_name = Hybrid_Test(Cifar10,Densenet,PT,lr0.001)
exp_name = 2. hybrid(all),cifar10,diff,n_data
